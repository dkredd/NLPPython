{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "686a067f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 11:15:39.115107: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e3b0bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18008896656']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = '''\n",
    "Contribute and support independent news channels. Consume a variety of sources of unbiased news.\n",
    "Verify information from multiple sources before forwarding, Dont assume everything you read is from a trusted source.\n",
    "Phone numbers to report fraudulent news articles are 18008896656 abd 1-(800)-887-8787\n",
    "'''\n",
    "\n",
    "pattern = \"\\d{11}\"\n",
    "matches = re.findall(pattern, text1)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a0d7e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['News consumption',\n",
       " 'Social Media Rules',\n",
       " 'Online bullying and flagging offensive content.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2 = '''\n",
    "Note 1 - News consumption\n",
    "Contribute and support independent news channels. Consume a variety of sources of unbiased news.\n",
    "Verify information from multiple sources before forwarding, Dont assume everything you read is from a trusted source.\n",
    "Phone numbers to report fraudulent news articles are 18008896656 abd 1-(800)-887-8787\n",
    "\n",
    "Note 2 - Social Media Rules\n",
    "When posting or commenting , keep discourse civil. Disagreements or opinions should be expressed as expressed \n",
    "in person, face to face and no special liberty should be taken because you are posting in an offline context.\n",
    "\n",
    "Note 3 - Online bullying and flagging offensive content.\n",
    "Make effort to combat online bullying by flagging offensive content to moderators or \n",
    "helping those who are bullied by offering supportive comments online. This is inline with what our kids are\n",
    "taught in schools : Be an upstander not a bystander.\n",
    "\n",
    "'''\n",
    "pattern = \"Note \\d - ([^\\n]*)\"\n",
    "matches = re.findall(pattern, text2)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03834bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021 Q2', '2020 Q2']\n",
      "[('2021 Q2', '158.08'), ('2020 Q2', '693')]\n"
     ]
    }
   ],
   "source": [
    "text3='''\n",
    "The Gross cost of operating medicare for all in fY2021 Q2 was $158.08 million. \n",
    "The previous year without the medicare for all option the gross cost in FY2020 Q2 was $693 million\n",
    "'''\n",
    "\n",
    "pattern = \"FY(\\d{4} Q[1-4])\"\n",
    "matches = re.findall(pattern, text3,flags=re.IGNORECASE)\n",
    "print(matches)\n",
    "pattern = \"FY(\\d{4} Q[1-4])[^\\$]+\\$([\\d.]+)\"\n",
    "matches = re.findall(pattern, text3,flags=re.IGNORECASE)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998dada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.Test visited Japan.\n",
      "[ Dr. ]\n",
      "[ Test ]\n",
      "[ visited ]\n",
      "[ Japan ]\n",
      "[ . ]\n",
      "Mr.Sam visited Turkey\n",
      "[ Mr. ]\n",
      "[ Sam ]\n",
      "[ visited ]\n",
      "[ Turkey ]\n"
     ]
    }
   ],
   "source": [
    "#check spacy vs nltk\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Dr.Test visited Japan. Mr.Sam visited Turkey\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for word in sent:\n",
    "        print(\"[\", word, \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd12d358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.Test visited Japan.', 'Mr.Sam visited Turkey']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"Dr.Test visited Japan. Mr.Sam visited Turkey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea9ba5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.Test', 'visited', 'Japan', '.', 'Mr.Sam', 'visited', 'Turkey']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Dr.Test visited Japan. Mr.Sam visited Turkey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ceb9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.lang.en.English'>\n",
      "Pipeline =  [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7fca1203b760>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7fca120c80a0>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7fca1150f740>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7fca12104e40>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7fca12112dc0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7fca120f2120>)]\n"
     ]
    }
   ],
   "source": [
    "#NLP Preprocessing\n",
    "#Tokenization: Sentence, Word then stemming, lemmatization\n",
    "print(type(nlp))\n",
    "print(\"Pipeline = \", nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80fe07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline =  []\n"
     ]
    }
   ],
   "source": [
    "#Test a blank pipeline with no tokens\n",
    "nlp = spacy.blank(\"en\")\n",
    "print(\"Pipeline = \", nlp.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "073867ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I Alpha - True Digit - False\n",
      "10 Alpha - False Digit - True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I gave 10 $ to Pet shelter.\")\n",
    "print(doc[0], \"Alpha -\",doc[0].is_alpha, \"Digit -\",doc[0].is_digit)\n",
    "print(doc[2], \"Alpha -\",doc[2].is_alpha, \"Digit -\",doc[2].is_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "549565a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme',\n",
       " 'more',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'checked',\n",
       " 'news',\n",
       " 'and',\n",
       " 'common',\n",
       " 'sense',\n",
       " 'opinions']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Customize your tokenizer\n",
    "doc = nlp(\"gimme more of the fact checked news and common sense opinions\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e1fbe60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim',\n",
       " 'me',\n",
       " 'more',\n",
       " 'of',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'checked',\n",
       " 'news',\n",
       " 'and',\n",
       " 'common',\n",
       " 'sense',\n",
       " 'opinions']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer.add_special_case(\"gimme\",[{ORTH:\"gim\"},{ORTH:\"me\"}])\n",
    "doc = nlp(\"gimme more of the fact checked news and common sense opinions\")\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1781fdf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/k8/gg_m5nbs2056hm824n8k0d3m0000gq/T/ipykernel_28832/1119897132.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dr.Test visited Japan. Mr.Sam visited Turkey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr.Test visited Japan. Mr.Sam visited Turkey\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for word in sent:\n",
    "        print(\"[\", word, \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238e7de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7fca1169b140>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NLP add to pipeline\n",
    "nlp.add_pipe('sentencizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cec2cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.Test visited Japan.\n",
      "[ Dr. ]\n",
      "[ Test ]\n",
      "[ visited ]\n",
      "[ Japan ]\n",
      "[ . ]\n",
      "Mr.Sam visited Turkey\n",
      "[ Mr. ]\n",
      "[ Sam ]\n",
      "[ visited ]\n",
      "[ Turkey ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr.Test visited Japan. Mr.Sam visited Turkey\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for word in sent:\n",
    "        print(\"[\", word, \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b7cd022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "भैया False\n",
      "जी False\n",
      "! False\n",
      "3000 False\n",
      "₹ True\n",
      "उधार False\n",
      "थे False\n",
      "वो False\n",
      "वापस False\n",
      "देदो False\n"
     ]
    }
   ],
   "source": [
    "#NLP with a new language\n",
    "nlp = spacy.blank(\"hi\")\n",
    "doc = nlp(\"भैया जी! 3000 ₹ उधार थे वो वापस देदो\")\n",
    "for token in doc:\n",
    "    print(token, token.is_currency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "940f66ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.data.gov/\n",
      "http://www.science\n",
      "http://data.gov.uk/.\n",
      "http://www3.norc.org/gss+website/\n",
      "http://www.europeansocialsurvey.org/.\n"
     ]
    }
   ],
   "source": [
    "#Extract the urls\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "\n",
    "doc = nlp(text)\n",
    "for sentence in doc.sents:\n",
    "    #print(sentence)\n",
    "    for word in sentence:\n",
    "        if(word.like_url): \n",
    "            print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3f629ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "#Extract the money transactions\n",
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
    "doc = nlp(transactions)\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency: \n",
    "        print(token.text, doc[token.i+1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8923a279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check pipeline Features\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ebbbd0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CaptainMontgomery | PROPN | CaptainMontgomery\n",
      "paid | VERB | pay\n",
      "200 | NUM | 200\n",
      "$ | NUM | $\n",
      "  | SPACE |  \n",
      "for | ADP | for\n",
      "trip | NOUN | trip\n",
      "to | ADP | to\n",
      "Italy | PROPN | Italy\n",
      ". | PUNCT | .\n",
      "He | PRON | he\n",
      "wanted | VERB | want\n",
      "to | PART | to\n",
      "tour | VERB | tour\n",
      "Australia | PROPN | Australia\n",
      "next | ADV | next\n",
      ". | PUNCT | .\n",
      "\n",
      "\n",
      "\n",
      "CaptainMontgomery\n",
      "200$\n",
      "Italy\n",
      "Australia\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CaptainMontgomery\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " paid \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    200$\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "  for trip to \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Italy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ". He wanted to tour \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Australia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " next.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(\"CaptainMontgomery paid 200$  for trip to Italy. He wanted to tour Australia next.\")\n",
    "for token in doc:\n",
    "    print(token, \"|\",token.pos_,\"|\",token.lemma_)\n",
    "    \n",
    "print(\"\\n\\n\")\n",
    "for ent in doc.ents:\n",
    "    print(ent)\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "503b6b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  PER  |  Named person or family.\n",
      "Twitter  |  MISC  |  Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23df6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
